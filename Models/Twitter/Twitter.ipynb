{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled162(5).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7722e0d64e1f4515a849783b8cd15311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_12f07a8fe566429c8e6771a671e46ecc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ad7fc882f68c43f18da7a044b2f41f26",
              "IPY_MODEL_f1d048f84bbb45f5b440e57da9e116d3"
            ]
          }
        },
        "12f07a8fe566429c8e6771a671e46ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad7fc882f68c43f18da7a044b2f41f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_52f265c8f9594096857450bf02aa74b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e87b5f5d08754aaab0303887df9195d9"
          }
        },
        "f1d048f84bbb45f5b440e57da9e116d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8c25c9d24c384b3fb58272b7f5256e03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/? [00:00&lt;00:00,  4.97it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5836ec33f6244324b9f1e1b052c5204b"
          }
        },
        "52f265c8f9594096857450bf02aa74b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e87b5f5d08754aaab0303887df9195d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c25c9d24c384b3fb58272b7f5256e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5836ec33f6244324b9f1e1b052c5204b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b5e55b6984f4a4392b6913a16bdbe9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4670ec7fb46c442e8d1ee704fa81f734",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2783e07d2774aacb4b0dd69b904ed1e",
              "IPY_MODEL_92c0d3bf3a7444dda05a9b1fd1c3280a"
            ]
          }
        },
        "4670ec7fb46c442e8d1ee704fa81f734": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2783e07d2774aacb4b0dd69b904ed1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6c62baa58ca747b682633142fdc3899e",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 8064,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f2b8e92828a74b19801cd2764269bc2b"
          }
        },
        "92c0d3bf3a7444dda05a9b1fd1c3280a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82c17fa7a59d43c585fc919de1d22544",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/8064 [02:10&lt;14:56:52,  6.67s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_59bee847824e4501aee60a9d345a465a"
          }
        },
        "6c62baa58ca747b682633142fdc3899e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f2b8e92828a74b19801cd2764269bc2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82c17fa7a59d43c585fc919de1d22544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "59bee847824e4501aee60a9d345a465a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdc119b2bfbc4186b4d7612a16644c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a405405c071b4e469039a8b52c7569d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8b027528a2ba460b81f23d6010e5dc10",
              "IPY_MODEL_2ab6478820e9425f93870016a0d770f0"
            ]
          }
        },
        "a405405c071b4e469039a8b52c7569d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b027528a2ba460b81f23d6010e5dc10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_619483aeb914413083cc670c12e53f3d",
            "_dom_classes": [],
            "description": " 14%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e19c64f343d41c98d1037ef03e03ba2"
          }
        },
        "2ab6478820e9425f93870016a0d770f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e032889234004eadaa8cc9839f7ebdae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/823 [01:56&lt;11:18,  1.05it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5961a942f829422b8c4988e17a2c4db7"
          }
        },
        "619483aeb914413083cc670c12e53f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e19c64f343d41c98d1037ef03e03ba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e032889234004eadaa8cc9839f7ebdae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5961a942f829422b8c4988e17a2c4db7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsamiei/step-one-dialogue/blob/master/Models/Twitter/Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkQPQqY6s8-v",
        "colab_type": "text"
      },
      "source": [
        "#In the name of God"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SudnGM-6qcaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80a89779-76cd-4192-9dc1-9f191622eadd"
      },
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        "  function ClickConnect(){\n",
        "    console.log(\"Working\"); \n",
        "    document.querySelector(\"colab-connect-button\").click() \n",
        "  }\n",
        "  var connect_timer = setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "  function ClickConnect(){\n",
              "    console.log(\"Working\"); \n",
              "    document.querySelector(\"colab-connect-button\").click() \n",
              "  }\n",
              "  var connect_timer = setInterval(ClickConnect,60000)\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTGb1dOrs48Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "f4f24b95-d81a-463e-8b33-e59150af7c0b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun 27 19:41:30 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyf240b5tD82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ee4f5c70-9274-4e38-b764-3684e3cdebb9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcbfW-tZtpLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip -q install transformers"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eweO40_dtxZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from transformers import AutoTokenizer\n",
        "import random\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import AutoModel"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X2E9kZ5tJRA",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eddoohNmtKug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file = '/content/drive/My Drive/Thesis/phase-3/twitter.csv'\n",
        "valid_file =  '/content/drive/My Drive/Thesis/phase-3/hkr_valid.csv'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgCX3Tv8tqPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-2_H-128_A-2')\n",
        "dec_tokenizer = AutoTokenizer.from_pretrained('google/bert_uncased_L-2_H-128_A-2')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Tn0rlhGQ0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(train_file)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oIRVi3aGTfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "4d46ca1f-6848-4b9f-8b05-d7ac8ebbd4cc"
      },
      "source": [
        "df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>psych is being taken off of netflix tomorrow a...</td>\n",
              "      <td>before you go to work open each episode up in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>car door was clearly not open if you look at t...</td>\n",
              "      <td>would you have sat and waited to see if he had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>i read the book also ! don't worry i'll be a g...</td>\n",
              "      <td>aww thanks chalupa i know i can trust your tas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>no were at the nats game like true patriots</td>\n",
              "      <td>the nats used to be canadian . try again .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>my twitter mentions are flying by like a 60 fp...</td>\n",
              "      <td>, please , please ! the confirm that you will ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2580428</th>\n",
              "      <td>2580428</td>\n",
              "      <td>only way she can get people to show up</td>\n",
              "      <td>crooked trump is unfit to lead america</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2580429</th>\n",
              "      <td>2580429</td>\n",
              "      <td>im glad my friend let me license transfer it b...</td>\n",
              "      <td>sameee lol it sucks tho because i had to pay 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2580430</th>\n",
              "      <td>2580430</td>\n",
              "      <td>no thing on me ( cocaine song ) by curtis mayf...</td>\n",
              "      <td>you still up too sis ? *sigh*</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2580431</th>\n",
              "      <td>2580431</td>\n",
              "      <td>q : \" what's your election night plan ? \" me :...</td>\n",
              "      <td>i plan on loading a bulliet myself</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2580432</th>\n",
              "      <td>2580432</td>\n",
              "      <td>shaggy mohombi faydee costi - habibi ( i need ...</td>\n",
              "      <td>i see you've fallen in love with 's music . is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2580433 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Unnamed: 0  ...                                              reply\n",
              "0                 0  ...  before you go to work open each episode up in ...\n",
              "1                 1  ...  would you have sat and waited to see if he had...\n",
              "2                 2  ...  aww thanks chalupa i know i can trust your tas...\n",
              "3                 3  ...         the nats used to be canadian . try again .\n",
              "4                 4  ...  , please , please ! the confirm that you will ...\n",
              "...             ...  ...                                                ...\n",
              "2580428     2580428  ...             crooked trump is unfit to lead america\n",
              "2580429     2580429  ...  sameee lol it sucks tho because i had to pay 9...\n",
              "2580430     2580430  ...                      you still up too sis ? *sigh*\n",
              "2580431     2580431  ...                 i plan on loading a bulliet myself\n",
              "2580432     2580432  ...  i see you've fallen in love with 's music . is...\n",
              "\n",
              "[2580433 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu21rVUPuIvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    \"\"\"My dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, frac=1, split_rate=1, max_len=512, sort=True, bound=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "        \"\"\"\n",
        "        self.dialogues = pd.read_csv(csv_file)\n",
        "        #self.dialogues.dropna(inplace=True)\n",
        "        \n",
        "        self.dialogues.fillna(\"\", inplace=True)\n",
        "        self.dialogues = self.dialogues[self.dialogues.index % split_rate == 0]\n",
        "\n",
        "        # s = self.dialogues['reply'].apply(dec_tokenizer.encode).apply(len).sort_values().index\n",
        "        # self.dialogues = self.dialogues.reindex(s)\n",
        "\n",
        "        # if bound:\n",
        "        #   len_prt = int(len(self.dialogues) / 5)\n",
        "        #   self.dialogues = self.dialogues[ : len_prt]\n",
        "\n",
        "        #self.dialogues.dropna(inplace=True)\n",
        "\n",
        "        self.max_len = max_len\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dialogues)\n",
        "\n",
        "    @staticmethod\n",
        "    def truncuate_join_pair_sentence(sentence1, sentence2, max_len=510):\n",
        "\n",
        "        \"\"\"\n",
        "        truncuate sentence one from head and sentence two from tail\n",
        "        Args:\n",
        "            sentence1 (string): first sentence\n",
        "            sentence2 (string): seconde sentence\n",
        "        \"\"\"\n",
        "        temp1 = enc_tokenizer.encode(sentence1,add_special_tokens=False)\n",
        "        temp2 = enc_tokenizer.encode(sentence2,add_special_tokens=False)\n",
        "        ### two above line may cause warning but no problem because we've handle them below\n",
        "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
        "        seq_1 = temp1\n",
        "        seq_2 = temp2\n",
        "        num_tokens_to_remove = len(temp1) + len(temp2) + 3 - max_len\n",
        "        if num_tokens_to_remove > 0 :\n",
        "            seq_1, seq_2, _ = enc_tokenizer.truncate_sequences(temp1[::-1],temp2, num_tokens_to_remove=num_tokens_to_remove)\n",
        "            seq_1.reverse()\n",
        "        result_list = [enc_tokenizer.cls_token_id]+seq_1+[enc_tokenizer.sep_token_id]+seq_2+[enc_tokenizer.sep_token_id]\n",
        "        token_type_ids = [0] * (len(seq_1) + 2) + [1] * (len(seq_2) + 1)\n",
        "        return result_list, token_type_ids\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      \n",
        "        \n",
        "        history = self.dialogues.iloc[idx].text\n",
        "        knowledge = \"\"\n",
        "        response = self.dialogues.iloc[idx].reply\n",
        "\n",
        "\n",
        "        input_pair, input_pair_segments = MyDataset.truncuate_join_pair_sentence(history, knowledge, self.max_len)\n",
        "                \n",
        "\n",
        "        input_pair = torch.LongTensor(input_pair)\n",
        "\n",
        "        input_pair_segments = torch.LongTensor(input_pair_segments)\n",
        "\n",
        "        response_tensor = torch.LongTensor(dec_tokenizer.encode(response, max_length=512))\n",
        "\n",
        "        sample = {'input_pair': input_pair,\n",
        "                  'input_pair_segments': input_pair_segments,\n",
        "                  'response': response_tensor}\n",
        "\n",
        "        return sample\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0jkglqFwFQb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "726bc630-7b2b-4d18-bee3-e0bb56d927cd"
      },
      "source": [
        "train_dataset = MyDataset(train_file, max_len=128, split_rate=5)\n",
        "valid_dataset = MyDataset(train_file, max_len=128, split_rate=49)\n",
        "print(len(train_dataset))\n",
        "print(len(valid_dataset))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "516087\n",
            "52662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VasXIkuLwHnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "93a0ba56-7bc8-4a71-84b7-d5d40719fd62"
      },
      "source": [
        "print(enc_tokenizer.decode(train_dataset[1000]['input_pair']))\n",
        "print(dec_tokenizer.decode(train_dataset[1000]['response']))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] looks great tommy.. amazing work : - ) [SEP] [SEP]\n",
            "[CLS] tyvm vicky! enjoy your sunday! xo [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Z5ZeAT2oii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227,
          "referenced_widgets": [
            "7722e0d64e1f4515a849783b8cd15311",
            "12f07a8fe566429c8e6771a671e46ecc",
            "ad7fc882f68c43f18da7a044b2f41f26",
            "f1d048f84bbb45f5b440e57da9e116d3",
            "52f265c8f9594096857450bf02aa74b5",
            "e87b5f5d08754aaab0303887df9195d9",
            "8c25c9d24c384b3fb58272b7f5256e03",
            "5836ec33f6244324b9f1e1b052c5204b"
          ]
        },
        "outputId": "2b8800c6-f484-425f-bc6e-e92ab47949a1"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "\n",
        "  len_batch = len(batch)\n",
        "\n",
        "  \n",
        "  max_len_input_pair = max([len(data['input_pair']) for data in batch])\n",
        "\n",
        "  max_len_response = max([len(data['response']) for data in batch])\n",
        "  \n",
        "  padding_ind = 0 ## for bert is 0 DON'T THINK BAD IT IS NOT REFACTORING !!!!!!\n",
        "  result_input_pair = torch.zeros(len_batch, max_len_input_pair)\n",
        "  result_input_pair_segments = torch.zeros(len_batch, max_len_input_pair)\n",
        "  result_response = torch.zeros(len_batch, max_len_response)\n",
        "\n",
        "  for i, data in enumerate(batch):\n",
        "    p1 = len(data['input_pair'])\n",
        "    result_input_pair[i, :p1] = data['input_pair']\n",
        "\n",
        "    p3 = len(data['input_pair_segments'])\n",
        "    result_input_pair_segments[i, :p3] = data['input_pair_segments']\n",
        "\n",
        "    p4 = len(data['response'])\n",
        "    result_response[i, :p4] = data['response']\n",
        "\n",
        "  return result_input_pair.long(), result_input_pair_segments.long(), result_response.long()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64,\n",
        "                                             shuffle=True, collate_fn=my_collate_fn,\n",
        "                                           num_workers=1)\n",
        "\n",
        "#valid_sampler = torch.utils.data.SequentialSampler(valid_dataset)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64,\n",
        "                                             shuffle=False, collate_fn=my_collate_fn, num_workers=1)\n",
        "\n",
        "i = 0 \n",
        "for batch_idx, batch  in tqdm(enumerate(train_loader)):\n",
        "  pair_batch, segment_batch, response_batch = batch\n",
        "  print(pair_batch.shape)\n",
        "  print(segment_batch.shape)\n",
        "  print(response_batch.shape)\n",
        "  print(\"****\")\n",
        "  i += 1 \n",
        "  if(i==2):\n",
        "    break\n",
        "\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(valid_loader))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7722e0d64e1f4515a849783b8cd15311",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 77])\n",
            "torch.Size([64, 77])\n",
            "torch.Size([64, 41])\n",
            "****\n",
            "torch.Size([64, 51])\n",
            "torch.Size([64, 51])\n",
            "torch.Size([64, 46])\n",
            "****\n",
            "8064\n",
            "823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoQlszJw_hFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "c5ef1ed1-4202-4916-d65d-d7cab8869c62"
      },
      "source": [
        "o = torch.rand(8,5)\n",
        "o"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7971, 0.5570, 0.6176, 0.8620, 0.0346],\n",
              "        [0.2617, 0.9808, 0.7682, 0.6243, 0.2782],\n",
              "        [0.4833, 0.6336, 0.0463, 0.0938, 0.4098],\n",
              "        [0.9999, 0.7067, 0.1404, 0.7106, 0.1492],\n",
              "        [0.0357, 0.5070, 0.1811, 0.8164, 0.6623],\n",
              "        [0.6191, 0.1194, 0.4503, 0.9537, 0.8837],\n",
              "        [0.2407, 0.3513, 0.4584, 0.2995, 0.2904],\n",
              "        [0.1673, 0.0891, 0.2090, 0.0751, 0.6567]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzwNQ5HwD5hg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0aec0d8-18f3-43e7-f137-27cc4551bceb"
      },
      "source": [
        "y = torch.LongTensor(8).random_(0,5)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 0, 0, 0, 4, 1, 4, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0fr2mAvGYA9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "749be628-2f4b-4379-c354-d490ccde15dd"
      },
      "source": [
        "o[y!=2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2617, 0.9808, 0.7682, 0.6243, 0.2782],\n",
              "        [0.4833, 0.6336, 0.0463, 0.0938, 0.4098],\n",
              "        [0.9999, 0.7067, 0.1404, 0.7106, 0.1492],\n",
              "        [0.0357, 0.5070, 0.1811, 0.8164, 0.6623],\n",
              "        [0.6191, 0.1194, 0.4503, 0.9537, 0.8837],\n",
              "        [0.2407, 0.3513, 0.4584, 0.2995, 0.2904]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxXzuZtMINMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cafb2b27-0aaf-4bb8-8d21-a759de573555"
      },
      "source": [
        "z = torch.LongTensor(o[y!=2].shape[0]).fill_(2)\n",
        "z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J49VS8z3Ilz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfWWb-ayBCsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "7132afe9-8373-49be-9a7b-225c4c8afa97"
      },
      "source": [
        "-1*F.nll_loss(nn.functional.log_softmax(o[y!=2]), z, reduction='mean')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.7754)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXB7YVVgDyPU",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfGvoJMiEicR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import EncoderDecoderModel, BertTokenizer\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "\n",
        "    self.seq2seq = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "        'google/bert_uncased_L-2_H-128_A-2', 'google/bert_uncased_L-2_H-128_A-2')\n",
        "    \n",
        "    for p in self.seq2seq.encoder.embeddings.parameters():\n",
        "       p.requires_grad = False\n",
        "    \n",
        "    #for p in self.seq2seq.decoder.embeddings.parameters():\n",
        "    #   p.requires_grad = False\n",
        "\n",
        "  def forward(self, encoder_input, segments_tensors, decoder_input):\n",
        "    '''\n",
        "    encoder_input = [batch_size, enc_len]\n",
        "    segments_tensors = [batch_size, enc_len]\n",
        "    decoder_input = [batch_size, dec_len]\n",
        "    '''\n",
        "    kwargs = {'token_type_ids':segments_tensors}\n",
        "    kwargs = {}\n",
        "    outputs = self.seq2seq(input_ids=encoder_input, decoder_input_ids=decoder_input, **kwargs)[0]\n",
        "    return outputs\n",
        "  \n",
        "  def generate(self, encoder_input, segments_tensors, **kwargs):\n",
        "    ### encoder_input = [len] in int format\n",
        "    ### segment_tensors = [len]\n",
        "    encoder_input = encoder_input.unsqueeze(0)\n",
        "    segments_tensors = segments_tensors.unsqueeze(0)\n",
        "    \n",
        "    #kwargs['token_type_ids'] = {'token_type_ids':segments_tensors}\n",
        "\n",
        "    generated = model.seq2seq.generate(encoder_input, decoder_start_token_id=101,\n",
        "                                       eos_token_id=102, ## [SEP] = 102\n",
        "                                       **kwargs)\n",
        "\n",
        "    #### generated = [1, len]\n",
        "    return generated"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxsRPOHFGrBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c662672a-9e2a-4663-9e55-b87ad63dabb1"
      },
      "source": [
        "dev = torch.device('cpu')\n",
        "model = Model().to(dev)\n",
        "\n",
        "# x = torch.LongTensor(200, 40).random_(1,1000).to(dev)\n",
        "# print(model(x).shape)\n",
        "\n",
        "\n",
        "def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(count_parameters(model))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4978874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb5uvuCKL0cj",
        "colab_type": "text"
      },
      "source": [
        "#Optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxB7zLBDZQXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufFe-Q62ZaFW",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-t7PADEZcZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn\n",
        "\n",
        "def mahdi_loss(model_output, true_trg, **kwargs):\n",
        "  '''\n",
        "  model_output: [batch, len, hidden]\n",
        "  true_trg: [batch, len]\n",
        "  '''\n",
        "  model_output = model_output[:,:-1,:]\n",
        "  true_trg = true_trg[:,1:]\n",
        "\n",
        "  # cold\n",
        "  #T = 1\n",
        "  #model_output = model_output / T\n",
        "\n",
        "  if 'easy_training' in kwargs:\n",
        "    limit_last_tokens = kwargs['easy_training']\n",
        "    model_output = model_output[:,-limit_last_tokens:,:]\n",
        "    true_trg = true_trg[:,-limit_last_tokens:]\n",
        "\n",
        "  batch_len = model_output.shape[0]\n",
        "  snt_len = model_output.shape[1]\n",
        "  hidden_size = model_output.shape[2]\n",
        "\n",
        "  model_output = model_output.reshape(-1, hidden_size)\n",
        "  true_trg = true_trg.reshape(-1)\n",
        "\n",
        "  loss_mod = nn.CrossEntropyLoss(ignore_index=0)## PAD = 0\n",
        "  loss = loss_mod(model_output, true_trg)\n",
        "\n",
        "\n",
        "\n",
        "  #z = torch.LongTensor(model_output[true_trg!=1045].shape[0]).fill_(1045).to(dev)\n",
        "  #neg_loss = -0.5*F.nll_loss(nn.functional.log_softmax(model_output[true_trg!=1045]), z, reduction='mean')\n",
        "\n",
        "  return loss "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmuTGJMJbR9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_step(batch_idx, batch):\n",
        "  pair_batch, segment_batch, response_batch = batch\n",
        "  pair_batch = pair_batch.to(dev)\n",
        "  segment_batch = segment_batch.to(dev)\n",
        "  response_batch = response_batch.to(dev)\n",
        "  model_output = model(pair_batch, segment_batch, response_batch)\n",
        "  #kwargs = {'easy_training':4}\n",
        "  loss = mahdi_loss(model_output, response_batch)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  del pair_batch\n",
        "  del segment_batch\n",
        "  del response_batch\n",
        "  return loss.item()\n",
        "\n",
        "def valid_step(batch_idx, batch):\n",
        "  with torch.no_grad():\n",
        "    pair_batch, segment_batch, response_batch = batch\n",
        "    pair_batch = pair_batch.to(dev)\n",
        "    segment_batch = segment_batch.to(dev)\n",
        "    response_batch = response_batch.to(dev)\n",
        "    model_output = model(pair_batch, segment_batch, response_batch)\n",
        "    loss = mahdi_loss(model_output, response_batch)\n",
        "    del pair_batch\n",
        "    del segment_batch\n",
        "    del response_batch\n",
        "    return loss.item()\n",
        "\n",
        "def valid_loop(valid_loader):\n",
        "  total_loss = 0\n",
        "  model.eval()\n",
        "  for batch_idx, batch in tqdm(enumerate(valid_loader),  total=len(valid_loader)):\n",
        "    total_loss += valid_step(batch_idx, batch)\n",
        "  \n",
        "  print(\"temperature is 1:\")\n",
        "  kwargs = {'num_beams':16,'num_return_sequences':16,'temperature':1}\n",
        "  valid_inference(**kwargs)\n",
        "\n",
        "  print(\"temperature is 0.33:\")\n",
        "  kwargs = {'num_beams':16,'num_return_sequences':16,'temperature':0.33}\n",
        "  valid_inference(**kwargs)\n",
        "\n",
        "  # print(\"temperature is 2:\")\n",
        "  # kwargs = {'num_beams':16,'num_return_sequences':16,'temperature':2}\n",
        "  # valid_inference(**kwargs)\n",
        "\n",
        "\n",
        "  model.train()\n",
        "  return total_loss / len(valid_loader)\n",
        "\n",
        "def valid_inference(idx=100, **kwargs):\n",
        "  hk_pair =  train_dataset[idx]['input_pair'].to(dev)\n",
        "  hk_segment = train_dataset[idx]['input_pair_segments'].to(dev)\n",
        "  response = train_dataset[idx]['response'].to(dev)\n",
        "  generateds = model.generate(hk_pair, hk_segment, **kwargs)\n",
        "  print(\"pair is: \",enc_tokenizer.decode(hk_pair))\n",
        "  print(\"response is: \",dec_tokenizer.decode(response))\n",
        "  for generated in generateds:\n",
        "    print(\"model says: \",dec_tokenizer.decode(generated))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pxjS0PQfKU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_learning = True\n",
        "if new_learning:\n",
        "  # optimizer = NoamOpt(128, 1, 2000,\n",
        "  #           torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "  model_dir = \"/content/drive/My Drive/Thesis/phase-3/Models/Twitter/\"\n",
        "  step = 0\n",
        "  log_list = []"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZD1hD7rfNFs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b619ef5b-b500-4a2a-e393-fb7c19e4531c"
      },
      "source": [
        "## if continue learning:\n",
        "#!wget -q https://github.com/mmsamiei/MS-Thesis-Phase2/raw/master/Models/hashemi_16000steps.model\n",
        "model_dir = \"/content/drive/My Drive/Thesis/phase-3/Models/Montazeri\"\n",
        "checkpoint = torch.load(model_dir+'/montazeri_15000steps.model')\n",
        "step = checkpoint['log_list'][-1]['step']\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "log_list = checkpoint['log_list']\n",
        "new_learning = False\n",
        "print(step)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHv6tC4YfZI9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563,
          "referenced_widgets": [
            "3b5e55b6984f4a4392b6913a16bdbe9e",
            "4670ec7fb46c442e8d1ee704fa81f734",
            "b2783e07d2774aacb4b0dd69b904ed1e",
            "92c0d3bf3a7444dda05a9b1fd1c3280a",
            "6c62baa58ca747b682633142fdc3899e",
            "f2b8e92828a74b19801cd2764269bc2b",
            "82c17fa7a59d43c585fc919de1d22544",
            "59bee847824e4501aee60a9d345a465a",
            "cdc119b2bfbc4186b4d7612a16644c26",
            "a405405c071b4e469039a8b52c7569d6",
            "8b027528a2ba460b81f23d6010e5dc10",
            "2ab6478820e9425f93870016a0d770f0",
            "619483aeb914413083cc670c12e53f3d",
            "0e19c64f343d41c98d1037ef03e03ba2",
            "e032889234004eadaa8cc9839f7ebdae",
            "5961a942f829422b8c4988e17a2c4db7"
          ]
        },
        "outputId": "65602c79-bc69-4672-da24-2ef86068a7c6"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "MAX_STEP = 20000\n",
        "STEP_SAVE = 2000\n",
        "STEP_CHECK = 3\n",
        "step_num = step + 1\n",
        "log_list = log_list ### Check if new learning or not\n",
        "print(step_num)\n",
        "while step_num <= MAX_STEP:\n",
        "  model.train()\n",
        "  for batch_idx, batch in tqdm(enumerate(iter(train_loader)), total=len(train_loader), leave=False):\n",
        "    step_loss = train_step(batch_idx, batch)\n",
        "    log = {'step':step_num, 'train_loss':step_loss}\n",
        "\n",
        "    if(step_num % STEP_CHECK == 0):\n",
        "      valid_error = valid_loop(valid_loader)\n",
        "      train_losses = [step['train_loss'] for step in log_list[-STEP_CHECK:]]\n",
        "      avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "      print(\"train Loss rate: {} at step {}\".format(avg_train_loss, step_num))  \n",
        "      print(\"valid Loss rate: {} at step {}\".format(valid_error, step_num))  \n",
        "      log['valid_loss'] = valid_error\n",
        "\n",
        "    log_list.append(log)\n",
        "\n",
        "    if(step_num % STEP_SAVE == 0):\n",
        "      torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'log_list': log_list,\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "            }, model_dir+'montazeri_{}steps.model'.format(step_num))\n",
        "    step_num += 1"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b5e55b6984f4a4392b6913a16bdbe9e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=8064.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdc119b2bfbc4186b4d7612a16644c26",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=823.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Yes\n",
            "torch.Size([64, 35])\n",
            "torch.Size([64, 35])\n",
            "torch.Size([64, 529])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-bb8984457c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSTEP_CHECK\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mvalid_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlog_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mSTEP_CHECK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-e03644bc131f>\u001b[0m in \u001b[0;36mvalid_loop\u001b[0;34m(valid_loader)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalid_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temperature is 1:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-e03644bc131f>\u001b[0m in \u001b[0;36mvalid_step\u001b[0;34m(batch_idx, batch)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msegment_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mresponse_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmahdi_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mpair_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-7d4b462b8215>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_input, segments_tensors, decoder_input)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msegments_tensors\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_encoder_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, inputs_embeds, attention_mask, head_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, decoder_head_mask, decoder_inputs_embeds, masked_lm_labels, lm_labels, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mlm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs_decoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         )\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, masked_lm_labels, encoder_hidden_states, encoder_attention_mask, lm_labels)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         )\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         embedding_output = self.embeddings(\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         )\n\u001b[1;32m    729\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow4c6BCePKec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "991374a6-a28d-4ab7-8515-543d13dca9a3"
      },
      "source": [
        "kwargs = {'num_beams':8,\n",
        "          'num_return_sequences':8,'temperature':1, 'max_length':50, 'early_stopping':True,\n",
        "          'no_repeat_ngram_size':3\n",
        "          }\n",
        "valid_inference(idx=1000, **kwargs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 102 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "pair is:  [CLS] yeah, but none of them use android they have their own custom os mostly. [SEP] initially developed by android inc., which google bought in 2005, android was unveiled in 2007, with the first commercial android device launched in september 2008. [SEP]\n",
            "response is:  [CLS] android was bought by google in 2005. [SEP]\n",
            "model says:  [CLS] i'm not sure, but i don't know that they've been around since 2007. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "model says:  [CLS] i'm not sure, but i don't know that they've been around since 2007, it was released in 2007. [SEP]\n",
            "model says:  [CLS] i'm not sure, but i don't know that they've been around since 2007, it was released in 2006. [SEP]\n",
            "model says:  [CLS] i'm not sure, but i don't know that they've been around since 2005. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "model says:  [CLS] i'm not sure, but i don't know that, but they've been around since 2007. [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "model says:  [CLS] i'm not sure, but i don't know that they've been around since 2007, it was released in 2005. [SEP]\n",
            "model says:  [CLS] i'm not sure, but i don't know that they've been around in 2007. [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\n",
            "model says:  [CLS] i'm not sure, but i don't know that they've been around since 2007, it was created in 2007. [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "001YzGTpEiV3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22f816d3-aa9b-4257-c6bf-f96fe6283f0f"
      },
      "source": [
        "log_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'step': 1, 'train_loss': 10.614270210266113},\n",
              " {'step': 2, 'train_loss': 10.733108520507812},\n",
              " {'step': 3, 'train_loss': 10.35904312133789},\n",
              " {'step': 4, 'train_loss': 10.361538887023926},\n",
              " {'step': 5, 'train_loss': 9.957269668579102},\n",
              " {'step': 6, 'train_loss': 9.948104858398438},\n",
              " {'step': 7, 'train_loss': 9.55443286895752},\n",
              " {'step': 8, 'train_loss': 9.504170417785645},\n",
              " {'step': 9, 'train_loss': 9.426634788513184},\n",
              " {'step': 10, 'train_loss': 9.070025444030762},\n",
              " {'step': 11, 'train_loss': 8.891423225402832},\n",
              " {'step': 12, 'train_loss': 8.783164024353027},\n",
              " {'step': 13, 'train_loss': 8.70628547668457},\n",
              " {'step': 14, 'train_loss': 8.569317817687988},\n",
              " {'step': 15, 'train_loss': 8.292956352233887},\n",
              " {'step': 16, 'train_loss': 8.274083137512207},\n",
              " {'step': 17, 'train_loss': 7.799631595611572},\n",
              " {'step': 18, 'train_loss': 8.041912078857422},\n",
              " {'step': 19, 'train_loss': 7.653815269470215},\n",
              " {'step': 20, 'train_loss': 7.699405670166016},\n",
              " {'step': 21, 'train_loss': 7.541056156158447},\n",
              " {'step': 22, 'train_loss': 7.494517803192139},\n",
              " {'step': 23, 'train_loss': 7.430841445922852},\n",
              " {'step': 24, 'train_loss': 7.282088279724121},\n",
              " {'step': 25, 'train_loss': 7.419764518737793},\n",
              " {'step': 26, 'train_loss': 7.311008453369141},\n",
              " {'step': 27, 'train_loss': 7.324136257171631},\n",
              " {'step': 28, 'train_loss': 7.19038724899292},\n",
              " {'step': 29, 'train_loss': 7.139859676361084},\n",
              " {'step': 30, 'train_loss': 6.9642133712768555},\n",
              " {'step': 31, 'train_loss': 7.110976219177246},\n",
              " {'step': 32, 'train_loss': 6.959036827087402},\n",
              " {'step': 33, 'train_loss': 6.991216659545898},\n",
              " {'step': 34, 'train_loss': 7.028300762176514},\n",
              " {'step': 35, 'train_loss': 6.890758514404297},\n",
              " {'step': 36, 'train_loss': 6.940341949462891},\n",
              " {'step': 37, 'train_loss': 6.829961776733398},\n",
              " {'step': 38, 'train_loss': 6.790741443634033},\n",
              " {'step': 39, 'train_loss': 6.722304344177246},\n",
              " {'step': 40, 'train_loss': 6.799952030181885},\n",
              " {'step': 41, 'train_loss': 6.738827705383301},\n",
              " {'step': 42, 'train_loss': 6.836117744445801},\n",
              " {'step': 43, 'train_loss': 6.828161239624023},\n",
              " {'step': 44, 'train_loss': 6.477605819702148},\n",
              " {'step': 45, 'train_loss': 6.66423225402832},\n",
              " {'step': 46, 'train_loss': 6.619389057159424},\n",
              " {'step': 47, 'train_loss': 6.610867977142334},\n",
              " {'step': 48, 'train_loss': 6.634395599365234},\n",
              " {'step': 49, 'train_loss': 6.7043023109436035},\n",
              " {'step': 50, 'train_loss': 6.447009086608887},\n",
              " {'step': 51, 'train_loss': 6.404237747192383},\n",
              " {'step': 52, 'train_loss': 6.602799415588379},\n",
              " {'step': 53, 'train_loss': 6.495773792266846},\n",
              " {'step': 54, 'train_loss': 6.4824700355529785},\n",
              " {'step': 55, 'train_loss': 6.533472537994385},\n",
              " {'step': 56, 'train_loss': 6.473059177398682},\n",
              " {'step': 57, 'train_loss': 6.536724090576172},\n",
              " {'step': 58, 'train_loss': 6.466060638427734},\n",
              " {'step': 59, 'train_loss': 6.412276268005371},\n",
              " {'step': 60, 'train_loss': 6.33018684387207},\n",
              " {'step': 61, 'train_loss': 6.287384986877441},\n",
              " {'step': 62, 'train_loss': 6.26254415512085},\n",
              " {'step': 63, 'train_loss': 6.33486795425415},\n",
              " {'step': 64, 'train_loss': 6.475207328796387},\n",
              " {'step': 65, 'train_loss': 6.321694374084473},\n",
              " {'step': 66, 'train_loss': 6.318795204162598},\n",
              " {'step': 67, 'train_loss': 6.250856876373291},\n",
              " {'step': 68, 'train_loss': 6.273882865905762},\n",
              " {'step': 69, 'train_loss': 6.177679061889648},\n",
              " {'step': 70, 'train_loss': 6.31804895401001},\n",
              " {'step': 71, 'train_loss': 6.312829494476318},\n",
              " {'step': 72, 'train_loss': 6.408702373504639},\n",
              " {'step': 73, 'train_loss': 6.233015060424805},\n",
              " {'step': 74, 'train_loss': 6.162512302398682},\n",
              " {'step': 75, 'train_loss': 6.125713348388672},\n",
              " {'step': 76, 'train_loss': 6.25309419631958},\n",
              " {'step': 77, 'train_loss': 6.191492557525635},\n",
              " {'step': 78, 'train_loss': 6.253152370452881},\n",
              " {'step': 79, 'train_loss': 6.144394397735596},\n",
              " {'step': 80, 'train_loss': 6.068164348602295},\n",
              " {'step': 81, 'train_loss': 6.047217845916748},\n",
              " {'step': 82, 'train_loss': 6.155714511871338},\n",
              " {'step': 83, 'train_loss': 6.2831292152404785},\n",
              " {'step': 84, 'train_loss': 6.0289506912231445},\n",
              " {'step': 85, 'train_loss': 6.046760082244873},\n",
              " {'step': 86, 'train_loss': 6.0089263916015625},\n",
              " {'step': 87, 'train_loss': 6.149424076080322},\n",
              " {'step': 88, 'train_loss': 5.892147064208984},\n",
              " {'step': 89, 'train_loss': 6.167072296142578},\n",
              " {'step': 90, 'train_loss': 6.048231601715088},\n",
              " {'step': 91, 'train_loss': 6.064284324645996},\n",
              " {'step': 92, 'train_loss': 6.100862503051758},\n",
              " {'step': 93, 'train_loss': 6.143848419189453},\n",
              " {'step': 94, 'train_loss': 5.9826250076293945},\n",
              " {'step': 95, 'train_loss': 6.055506706237793},\n",
              " {'step': 96, 'train_loss': 6.1009111404418945},\n",
              " {'step': 97, 'train_loss': 5.9516167640686035},\n",
              " {'step': 98, 'train_loss': 5.892017841339111},\n",
              " {'step': 99, 'train_loss': 6.066226482391357},\n",
              " {'step': 100, 'train_loss': 6.033585071563721},\n",
              " {'step': 101, 'train_loss': 5.9667582511901855},\n",
              " {'step': 102, 'train_loss': 5.870423793792725},\n",
              " {'step': 103, 'train_loss': 5.994822025299072},\n",
              " {'step': 104, 'train_loss': 5.971651554107666},\n",
              " {'step': 105, 'train_loss': 5.9873247146606445},\n",
              " {'step': 106, 'train_loss': 6.104528903961182},\n",
              " {'step': 107, 'train_loss': 6.018568515777588},\n",
              " {'step': 108, 'train_loss': 5.905545711517334},\n",
              " {'step': 109, 'train_loss': 5.904337406158447},\n",
              " {'step': 110, 'train_loss': 5.857326984405518},\n",
              " {'step': 111, 'train_loss': 5.910941123962402},\n",
              " {'step': 112, 'train_loss': 5.952322959899902},\n",
              " {'step': 113, 'train_loss': 5.930358409881592},\n",
              " {'step': 114, 'train_loss': 5.820103645324707},\n",
              " {'step': 115, 'train_loss': 5.904860496520996},\n",
              " {'step': 116, 'train_loss': 5.746059894561768},\n",
              " {'step': 117, 'train_loss': 5.919399738311768},\n",
              " {'step': 118, 'train_loss': 5.721978664398193},\n",
              " {'step': 119, 'train_loss': 5.978499412536621},\n",
              " {'step': 120, 'train_loss': 5.803157806396484},\n",
              " {'step': 121, 'train_loss': 5.9035115242004395},\n",
              " {'step': 122, 'train_loss': 5.905656337738037},\n",
              " {'step': 123, 'train_loss': 5.965410232543945},\n",
              " {'step': 124, 'train_loss': 5.693082332611084},\n",
              " {'step': 125, 'train_loss': 5.800467491149902},\n",
              " {'step': 126, 'train_loss': 5.9416961669921875},\n",
              " {'step': 127, 'train_loss': 5.744022846221924},\n",
              " {'step': 128, 'train_loss': 5.756189823150635},\n",
              " {'step': 129, 'train_loss': 5.972407341003418},\n",
              " {'step': 130, 'train_loss': 5.739309787750244},\n",
              " {'step': 131, 'train_loss': 5.773653984069824},\n",
              " {'step': 132, 'train_loss': 5.7204270362854},\n",
              " {'step': 133, 'train_loss': 5.745061874389648},\n",
              " {'step': 134, 'train_loss': 5.694537162780762},\n",
              " {'step': 135, 'train_loss': 5.807111740112305},\n",
              " {'step': 136, 'train_loss': 5.677490234375},\n",
              " {'step': 137, 'train_loss': 5.84185791015625},\n",
              " {'step': 138, 'train_loss': 5.757626056671143},\n",
              " {'step': 139, 'train_loss': 5.838588237762451},\n",
              " {'step': 140, 'train_loss': 5.887659072875977},\n",
              " {'step': 141, 'train_loss': 5.799559116363525},\n",
              " {'step': 142, 'train_loss': 5.857546806335449},\n",
              " {'step': 143, 'train_loss': 5.841247081756592},\n",
              " {'step': 144, 'train_loss': 5.7828192710876465},\n",
              " {'step': 145, 'train_loss': 5.7508111000061035},\n",
              " {'step': 146, 'train_loss': 5.657776355743408},\n",
              " {'step': 147, 'train_loss': 5.773700714111328},\n",
              " {'step': 148, 'train_loss': 5.783103942871094},\n",
              " {'step': 149, 'train_loss': 5.6356306076049805},\n",
              " {'step': 150, 'train_loss': 5.7269606590271},\n",
              " {'step': 151, 'train_loss': 5.664881229400635},\n",
              " {'step': 152, 'train_loss': 5.8323140144348145},\n",
              " {'step': 153, 'train_loss': 5.798088073730469},\n",
              " {'step': 154, 'train_loss': 5.622040748596191},\n",
              " {'step': 155, 'train_loss': 5.643208980560303},\n",
              " {'step': 156, 'train_loss': 5.625682353973389},\n",
              " {'step': 157, 'train_loss': 5.750513076782227},\n",
              " {'step': 158, 'train_loss': 5.640925884246826},\n",
              " {'step': 159, 'train_loss': 5.544066905975342},\n",
              " {'step': 160, 'train_loss': 5.526726245880127},\n",
              " {'step': 161, 'train_loss': 5.635190963745117},\n",
              " {'step': 162, 'train_loss': 5.542055606842041},\n",
              " {'step': 163, 'train_loss': 5.692262172698975},\n",
              " {'step': 164, 'train_loss': 5.516684532165527},\n",
              " {'step': 165, 'train_loss': 5.639286518096924},\n",
              " {'step': 166, 'train_loss': 5.5649003982543945},\n",
              " {'step': 167, 'train_loss': 5.659287452697754},\n",
              " {'step': 168, 'train_loss': 5.723378658294678},\n",
              " {'step': 169, 'train_loss': 5.550490379333496},\n",
              " {'step': 170, 'train_loss': 5.501671314239502},\n",
              " {'step': 171, 'train_loss': 5.554276943206787},\n",
              " {'step': 172, 'train_loss': 5.663266658782959},\n",
              " {'step': 173, 'train_loss': 5.390931129455566},\n",
              " {'step': 174, 'train_loss': 5.539400100708008},\n",
              " {'step': 175, 'train_loss': 5.562276840209961},\n",
              " {'step': 176, 'train_loss': 5.4964823722839355},\n",
              " {'step': 177, 'train_loss': 5.461878299713135},\n",
              " {'step': 178, 'train_loss': 5.557440757751465},\n",
              " {'step': 179, 'train_loss': 5.599611282348633},\n",
              " {'step': 180, 'train_loss': 5.491677761077881},\n",
              " {'step': 181, 'train_loss': 5.663024425506592},\n",
              " {'step': 182, 'train_loss': 5.459636211395264},\n",
              " {'step': 183, 'train_loss': 5.422452449798584},\n",
              " {'step': 184, 'train_loss': 5.511804580688477},\n",
              " {'step': 185, 'train_loss': 5.4484710693359375},\n",
              " {'step': 186, 'train_loss': 5.597130298614502},\n",
              " {'step': 187, 'train_loss': 5.456928730010986},\n",
              " {'step': 188, 'train_loss': 5.553258895874023},\n",
              " {'step': 189, 'train_loss': 5.411586284637451},\n",
              " {'step': 190, 'train_loss': 5.478546619415283},\n",
              " {'step': 191, 'train_loss': 5.59134578704834},\n",
              " {'step': 192, 'train_loss': 5.498397350311279},\n",
              " {'step': 193, 'train_loss': 5.555095672607422},\n",
              " {'step': 194, 'train_loss': 5.665139198303223},\n",
              " {'step': 195, 'train_loss': 5.599613189697266},\n",
              " {'step': 196, 'train_loss': 5.510522842407227},\n",
              " {'step': 197, 'train_loss': 5.527666091918945},\n",
              " {'step': 198, 'train_loss': 5.446256160736084},\n",
              " {'step': 199, 'train_loss': 5.387380123138428},\n",
              " {'step': 200, 'train_loss': 5.571355819702148},\n",
              " {'step': 201, 'train_loss': 5.513912677764893},\n",
              " {'step': 202, 'train_loss': 5.607326030731201},\n",
              " {'step': 203, 'train_loss': 5.477540016174316},\n",
              " {'step': 204, 'train_loss': 5.47022819519043},\n",
              " {'step': 205, 'train_loss': 5.48403263092041},\n",
              " {'step': 206, 'train_loss': 5.518249988555908},\n",
              " {'step': 207, 'train_loss': 5.512309551239014},\n",
              " {'step': 208, 'train_loss': 5.531373023986816},\n",
              " {'step': 209, 'train_loss': 5.429182052612305},\n",
              " {'step': 210, 'train_loss': 5.482989311218262},\n",
              " {'step': 211, 'train_loss': 5.593931198120117},\n",
              " {'step': 212, 'train_loss': 5.483344554901123},\n",
              " {'step': 213, 'train_loss': 5.4552903175354},\n",
              " {'step': 214, 'train_loss': 5.425174713134766},\n",
              " {'step': 215, 'train_loss': 5.394619941711426},\n",
              " {'step': 216, 'train_loss': 5.496830940246582},\n",
              " {'step': 217, 'train_loss': 5.533327102661133},\n",
              " {'step': 218, 'train_loss': 5.4912238121032715},\n",
              " {'step': 219, 'train_loss': 5.562330722808838},\n",
              " {'step': 220, 'train_loss': 5.304871082305908},\n",
              " {'step': 221, 'train_loss': 5.3457746505737305},\n",
              " {'step': 222, 'train_loss': 5.41758918762207},\n",
              " {'step': 223, 'train_loss': 5.411736011505127},\n",
              " {'step': 224, 'train_loss': 5.412827014923096},\n",
              " {'step': 225, 'train_loss': 5.3450517654418945},\n",
              " {'step': 226, 'train_loss': 5.35407829284668},\n",
              " {'step': 227, 'train_loss': 5.5518341064453125},\n",
              " {'step': 228, 'train_loss': 5.4043169021606445},\n",
              " {'step': 229, 'train_loss': 5.436760902404785},\n",
              " {'step': 230, 'train_loss': 5.48104190826416},\n",
              " {'step': 231, 'train_loss': 5.299251079559326},\n",
              " {'step': 232, 'train_loss': 5.580989360809326},\n",
              " {'step': 233, 'train_loss': 5.41370153427124},\n",
              " {'step': 234, 'train_loss': 5.181121826171875},\n",
              " {'step': 235, 'train_loss': 5.367734432220459},\n",
              " {'step': 236, 'train_loss': 5.275777339935303},\n",
              " {'step': 237, 'train_loss': 5.550502300262451},\n",
              " {'step': 238, 'train_loss': 5.493572235107422},\n",
              " {'step': 239, 'train_loss': 5.453638553619385},\n",
              " {'step': 240, 'train_loss': 5.390020370483398},\n",
              " {'step': 241, 'train_loss': 5.64306640625},\n",
              " {'step': 242, 'train_loss': 5.400513648986816},\n",
              " {'step': 243, 'train_loss': 5.425976276397705},\n",
              " {'step': 244, 'train_loss': 5.3698296546936035},\n",
              " {'step': 245, 'train_loss': 5.198696613311768},\n",
              " {'step': 246, 'train_loss': 5.209316730499268},\n",
              " {'step': 247, 'train_loss': 5.377569675445557},\n",
              " {'step': 248, 'train_loss': 5.4149627685546875},\n",
              " {'step': 249, 'train_loss': 5.3666205406188965},\n",
              " {'step': 250, 'train_loss': 5.333164215087891},\n",
              " {'step': 251, 'train_loss': 5.156434059143066},\n",
              " {'step': 252, 'train_loss': 5.3128461837768555},\n",
              " {'step': 253, 'train_loss': 5.443748950958252},\n",
              " {'step': 254, 'train_loss': 5.282620906829834},\n",
              " {'step': 255, 'train_loss': 5.2941083908081055},\n",
              " {'step': 256, 'train_loss': 5.236748695373535},\n",
              " {'step': 257, 'train_loss': 5.346299171447754},\n",
              " {'step': 258, 'train_loss': 5.353781700134277},\n",
              " {'step': 259, 'train_loss': 5.340931415557861},\n",
              " {'step': 260, 'train_loss': 5.228150367736816},\n",
              " {'step': 261, 'train_loss': 5.342605113983154},\n",
              " {'step': 262, 'train_loss': 5.388302326202393},\n",
              " {'step': 263, 'train_loss': 5.460541248321533},\n",
              " {'step': 264, 'train_loss': 5.427804470062256},\n",
              " {'step': 265, 'train_loss': 5.254043102264404},\n",
              " {'step': 266, 'train_loss': 5.221750259399414},\n",
              " {'step': 267, 'train_loss': 5.38762903213501},\n",
              " {'step': 268, 'train_loss': 5.255697727203369},\n",
              " {'step': 269, 'train_loss': 5.259950637817383},\n",
              " {'step': 270, 'train_loss': 5.17145299911499},\n",
              " {'step': 271, 'train_loss': 5.386752605438232},\n",
              " {'step': 272, 'train_loss': 5.239328861236572},\n",
              " {'step': 273, 'train_loss': 5.372757911682129},\n",
              " {'step': 274, 'train_loss': 5.205066680908203},\n",
              " {'step': 275, 'train_loss': 5.192701816558838},\n",
              " {'step': 276, 'train_loss': 5.143523216247559},\n",
              " {'step': 277, 'train_loss': 5.302921772003174},\n",
              " {'step': 278, 'train_loss': 5.202968120574951},\n",
              " {'step': 279, 'train_loss': 5.311514377593994},\n",
              " {'step': 280, 'train_loss': 5.29379940032959},\n",
              " {'step': 281, 'train_loss': 5.106523036956787},\n",
              " {'step': 282, 'train_loss': 5.1381611824035645},\n",
              " {'step': 283, 'train_loss': 5.278992652893066},\n",
              " {'step': 284, 'train_loss': 5.234774589538574},\n",
              " {'step': 285, 'train_loss': 5.253359317779541},\n",
              " {'step': 286, 'train_loss': 5.254481315612793},\n",
              " {'step': 287, 'train_loss': 5.230638027191162},\n",
              " {'step': 288, 'train_loss': 5.305098056793213},\n",
              " {'step': 289, 'train_loss': 5.227379322052002},\n",
              " {'step': 290, 'train_loss': 5.2933573722839355},\n",
              " {'step': 291, 'train_loss': 5.462745189666748},\n",
              " {'step': 292, 'train_loss': 5.202146530151367},\n",
              " {'step': 293, 'train_loss': 5.285743713378906},\n",
              " {'step': 294, 'train_loss': 5.336248397827148},\n",
              " {'step': 295, 'train_loss': 5.283276557922363},\n",
              " {'step': 296, 'train_loss': 5.275354862213135},\n",
              " {'step': 297, 'train_loss': 5.258694171905518},\n",
              " {'step': 298, 'train_loss': 5.278975963592529},\n",
              " {'step': 299, 'train_loss': 5.130582809448242},\n",
              " {'step': 300, 'train_loss': 5.3244099617004395},\n",
              " {'step': 301, 'train_loss': 5.316249847412109},\n",
              " {'step': 302, 'train_loss': 5.144835948944092},\n",
              " {'step': 303, 'train_loss': 5.181732654571533},\n",
              " {'step': 304, 'train_loss': 5.234304428100586},\n",
              " {'step': 305, 'train_loss': 5.360167980194092},\n",
              " {'step': 306, 'train_loss': 5.211554050445557},\n",
              " {'step': 307, 'train_loss': 5.216710567474365},\n",
              " {'step': 308, 'train_loss': 5.191037178039551},\n",
              " {'step': 309, 'train_loss': 5.253116607666016},\n",
              " {'step': 310, 'train_loss': 5.272846698760986},\n",
              " {'step': 311, 'train_loss': 5.222359657287598},\n",
              " {'step': 312, 'train_loss': 5.204648017883301},\n",
              " {'step': 313, 'train_loss': 5.246396064758301},\n",
              " {'step': 314, 'train_loss': 5.259006977081299},\n",
              " {'step': 315, 'train_loss': 5.206407070159912},\n",
              " {'step': 316, 'train_loss': 5.08339262008667},\n",
              " {'step': 317, 'train_loss': 5.295568466186523},\n",
              " {'step': 318, 'train_loss': 5.240293025970459},\n",
              " {'step': 319, 'train_loss': 5.213573455810547},\n",
              " {'step': 320, 'train_loss': 4.895663261413574},\n",
              " {'step': 321, 'train_loss': 5.50667667388916},\n",
              " {'step': 322, 'train_loss': 5.187763690948486},\n",
              " {'step': 323, 'train_loss': 5.250135898590088},\n",
              " {'step': 324, 'train_loss': 5.1063313484191895},\n",
              " {'step': 325, 'train_loss': 5.174783229827881},\n",
              " {'step': 326, 'train_loss': 5.233711242675781},\n",
              " {'step': 327, 'train_loss': 5.135952472686768},\n",
              " {'step': 328, 'train_loss': 5.156679630279541},\n",
              " {'step': 329, 'train_loss': 5.114612579345703},\n",
              " {'step': 330, 'train_loss': 5.157589912414551},\n",
              " {'step': 331, 'train_loss': 5.21737813949585},\n",
              " {'step': 332, 'train_loss': 5.2145538330078125},\n",
              " {'step': 333, 'train_loss': 5.105071067810059},\n",
              " {'step': 334, 'train_loss': 5.173450946807861},\n",
              " {'step': 335, 'train_loss': 5.268897533416748},\n",
              " {'step': 336, 'train_loss': 5.095746994018555},\n",
              " {'step': 337, 'train_loss': 5.074581623077393},\n",
              " {'step': 338, 'train_loss': 5.216559886932373},\n",
              " {'step': 339, 'train_loss': 5.231313228607178},\n",
              " {'step': 340, 'train_loss': 5.179142475128174},\n",
              " {'step': 341, 'train_loss': 5.070371627807617},\n",
              " {'step': 342, 'train_loss': 5.1507720947265625},\n",
              " {'step': 343, 'train_loss': 5.125454425811768},\n",
              " {'step': 344, 'train_loss': 5.247156620025635},\n",
              " {'step': 345, 'train_loss': 4.955747127532959},\n",
              " {'step': 346, 'train_loss': 5.198351860046387},\n",
              " {'step': 347, 'train_loss': 5.244842052459717},\n",
              " {'step': 348, 'train_loss': 5.1056013107299805},\n",
              " {'step': 349, 'train_loss': 5.222116470336914},\n",
              " {'step': 350, 'train_loss': 5.171998023986816},\n",
              " {'step': 351, 'train_loss': 5.087792873382568},\n",
              " {'step': 352, 'train_loss': 5.151352882385254},\n",
              " {'step': 353, 'train_loss': 5.072177886962891},\n",
              " {'step': 354, 'train_loss': 5.1587090492248535},\n",
              " {'step': 355, 'train_loss': 5.172445297241211},\n",
              " {'step': 356, 'train_loss': 5.290968418121338},\n",
              " {'step': 357, 'train_loss': 5.2038116455078125},\n",
              " {'step': 358, 'train_loss': 5.205150604248047},\n",
              " {'step': 359, 'train_loss': 5.112959861755371},\n",
              " {'step': 360, 'train_loss': 5.100018501281738},\n",
              " {'step': 361, 'train_loss': 5.30513858795166},\n",
              " {'step': 362, 'train_loss': 5.061429500579834},\n",
              " {'step': 363, 'train_loss': 5.306804180145264},\n",
              " {'step': 364, 'train_loss': 5.22298526763916},\n",
              " {'step': 365, 'train_loss': 4.979709148406982},\n",
              " {'step': 366, 'train_loss': 5.270254135131836},\n",
              " {'step': 367, 'train_loss': 5.341934680938721},\n",
              " {'step': 368, 'train_loss': 5.328076362609863},\n",
              " {'step': 369, 'train_loss': 5.200632095336914},\n",
              " {'step': 370, 'train_loss': 5.0119218826293945},\n",
              " {'step': 371, 'train_loss': 5.029152870178223},\n",
              " {'step': 372, 'train_loss': 5.143714904785156},\n",
              " {'step': 373, 'train_loss': 5.10402774810791},\n",
              " {'step': 374, 'train_loss': 5.082812309265137},\n",
              " {'step': 375, 'train_loss': 5.298189163208008},\n",
              " {'step': 376, 'train_loss': 5.0597686767578125},\n",
              " {'step': 377, 'train_loss': 5.093031406402588},\n",
              " {'step': 378, 'train_loss': 5.162484169006348},\n",
              " {'step': 379, 'train_loss': 5.286736488342285},\n",
              " {'step': 380, 'train_loss': 5.101831436157227},\n",
              " {'step': 381, 'train_loss': 5.166261672973633},\n",
              " {'step': 382, 'train_loss': 5.011640548706055},\n",
              " {'step': 383, 'train_loss': 5.035333633422852},\n",
              " {'step': 384, 'train_loss': 5.036679744720459},\n",
              " {'step': 385, 'train_loss': 5.306996822357178},\n",
              " {'step': 386, 'train_loss': 5.0076003074646},\n",
              " {'step': 387, 'train_loss': 5.093479633331299},\n",
              " {'step': 388, 'train_loss': 4.976170063018799},\n",
              " {'step': 389, 'train_loss': 4.916290760040283},\n",
              " {'step': 390, 'train_loss': 5.040968418121338},\n",
              " {'step': 391, 'train_loss': 5.125514507293701},\n",
              " {'step': 392, 'train_loss': 5.093578338623047},\n",
              " {'step': 393, 'train_loss': 5.078884124755859},\n",
              " {'step': 394, 'train_loss': 5.019901275634766},\n",
              " {'step': 395, 'train_loss': 5.083425045013428},\n",
              " {'step': 396, 'train_loss': 5.183268070220947},\n",
              " {'step': 397, 'train_loss': 5.118241786956787},\n",
              " {'step': 398, 'train_loss': 5.050731658935547},\n",
              " {'step': 399, 'train_loss': 5.223462104797363},\n",
              " {'step': 400, 'train_loss': 5.006255149841309},\n",
              " {'step': 401, 'train_loss': 5.163806915283203},\n",
              " {'step': 402, 'train_loss': 5.151505947113037},\n",
              " {'step': 403, 'train_loss': 5.0700225830078125},\n",
              " {'step': 404, 'train_loss': 5.098755836486816},\n",
              " {'step': 405, 'train_loss': 5.1258015632629395},\n",
              " {'step': 406, 'train_loss': 5.315492630004883},\n",
              " {'step': 407, 'train_loss': 5.060413360595703},\n",
              " {'step': 408, 'train_loss': 5.063150405883789},\n",
              " {'step': 409, 'train_loss': 5.16451358795166},\n",
              " {'step': 410, 'train_loss': 4.921462535858154},\n",
              " {'step': 411, 'train_loss': 5.141750812530518},\n",
              " {'step': 412, 'train_loss': 5.0962958335876465},\n",
              " {'step': 413, 'train_loss': 5.095948219299316},\n",
              " {'step': 414, 'train_loss': 5.161494731903076},\n",
              " {'step': 415, 'train_loss': 5.221571445465088},\n",
              " {'step': 416, 'train_loss': 5.08030366897583},\n",
              " {'step': 417, 'train_loss': 5.055623531341553},\n",
              " {'step': 418, 'train_loss': 5.023617267608643},\n",
              " {'step': 419, 'train_loss': 5.07837438583374},\n",
              " {'step': 420, 'train_loss': 5.092581748962402},\n",
              " {'step': 421, 'train_loss': 5.072193622589111},\n",
              " {'step': 422, 'train_loss': 5.03213357925415},\n",
              " {'step': 423, 'train_loss': 5.305527687072754},\n",
              " {'step': 424, 'train_loss': 5.157072067260742},\n",
              " {'step': 425, 'train_loss': 5.059497356414795},\n",
              " {'step': 426, 'train_loss': 5.082494258880615},\n",
              " {'step': 427, 'train_loss': 5.146284103393555},\n",
              " {'step': 428, 'train_loss': 5.144371032714844},\n",
              " {'step': 429, 'train_loss': 5.043665885925293},\n",
              " {'step': 430, 'train_loss': 5.044711589813232},\n",
              " {'step': 431, 'train_loss': 5.031794548034668},\n",
              " {'step': 432, 'train_loss': 5.334057807922363},\n",
              " {'step': 433, 'train_loss': 5.106303691864014},\n",
              " {'step': 434, 'train_loss': 5.100714683532715},\n",
              " {'step': 435, 'train_loss': 5.054747104644775},\n",
              " {'step': 436, 'train_loss': 4.94317626953125},\n",
              " {'step': 437, 'train_loss': 4.928979873657227},\n",
              " {'step': 438, 'train_loss': 4.9707350730896},\n",
              " {'step': 439, 'train_loss': 5.006022930145264},\n",
              " {'step': 440, 'train_loss': 5.081789493560791},\n",
              " {'step': 441, 'train_loss': 5.076113224029541},\n",
              " {'step': 442, 'train_loss': 5.085782527923584},\n",
              " {'step': 443, 'train_loss': 4.937224388122559},\n",
              " {'step': 444, 'train_loss': 4.804088592529297},\n",
              " {'step': 445, 'train_loss': 5.01839017868042},\n",
              " {'step': 446, 'train_loss': 5.025193691253662},\n",
              " {'step': 447, 'train_loss': 4.9998626708984375},\n",
              " {'step': 448, 'train_loss': 5.040356159210205},\n",
              " {'step': 449, 'train_loss': 5.091433048248291},\n",
              " {'step': 450, 'train_loss': 5.015182018280029},\n",
              " {'step': 451, 'train_loss': 4.892190933227539},\n",
              " {'step': 452, 'train_loss': 4.899415016174316},\n",
              " {'step': 453, 'train_loss': 5.003859519958496},\n",
              " {'step': 454, 'train_loss': 5.149337291717529},\n",
              " {'step': 455, 'train_loss': 5.0463948249816895},\n",
              " {'step': 456, 'train_loss': 4.990828514099121},\n",
              " {'step': 457, 'train_loss': 5.085023403167725},\n",
              " {'step': 458, 'train_loss': 5.0593767166137695},\n",
              " {'step': 459, 'train_loss': 5.015804290771484},\n",
              " {'step': 460, 'train_loss': 5.101353168487549},\n",
              " {'step': 461, 'train_loss': 5.027591705322266},\n",
              " {'step': 462, 'train_loss': 5.019528865814209},\n",
              " {'step': 463, 'train_loss': 4.984124660491943},\n",
              " {'step': 464, 'train_loss': 4.940833568572998},\n",
              " {'step': 465, 'train_loss': 4.920216083526611},\n",
              " {'step': 466, 'train_loss': 4.989704132080078},\n",
              " {'step': 467, 'train_loss': 4.992586135864258},\n",
              " {'step': 468, 'train_loss': 5.075007438659668},\n",
              " {'step': 469, 'train_loss': 5.003719329833984},\n",
              " {'step': 470, 'train_loss': 4.943905353546143},\n",
              " {'step': 471, 'train_loss': 5.14833402633667},\n",
              " {'step': 472, 'train_loss': 5.0780720710754395},\n",
              " {'step': 473, 'train_loss': 4.882382392883301},\n",
              " {'step': 474, 'train_loss': 5.2562127113342285},\n",
              " {'step': 475, 'train_loss': 4.818055629730225},\n",
              " {'step': 476, 'train_loss': 5.037609577178955},\n",
              " {'step': 477, 'train_loss': 5.123337745666504},\n",
              " {'step': 478, 'train_loss': 4.985408306121826},\n",
              " {'step': 479, 'train_loss': 4.921868801116943},\n",
              " {'step': 480, 'train_loss': 4.8449320793151855},\n",
              " {'step': 481, 'train_loss': 5.036040782928467},\n",
              " {'step': 482, 'train_loss': 4.898875713348389},\n",
              " {'step': 483, 'train_loss': 5.009551525115967},\n",
              " {'step': 484, 'train_loss': 4.94397497177124},\n",
              " {'step': 485, 'train_loss': 5.13902473449707},\n",
              " {'step': 486, 'train_loss': 5.0791544914245605},\n",
              " {'step': 487, 'train_loss': 4.988691806793213},\n",
              " {'step': 488, 'train_loss': 5.013467788696289},\n",
              " {'step': 489, 'train_loss': 5.023211479187012},\n",
              " {'step': 490, 'train_loss': 4.9566168785095215},\n",
              " {'step': 491, 'train_loss': 5.128715991973877},\n",
              " {'step': 492, 'train_loss': 4.930099010467529},\n",
              " {'step': 493, 'train_loss': 4.9543046951293945},\n",
              " {'step': 494, 'train_loss': 5.083960056304932},\n",
              " {'step': 495, 'train_loss': 5.027308464050293},\n",
              " {'step': 496, 'train_loss': 5.060235977172852},\n",
              " {'step': 497, 'train_loss': 4.944437503814697},\n",
              " {'step': 498, 'train_loss': 4.9654765129089355},\n",
              " {'step': 499, 'train_loss': 5.12076997756958},\n",
              " {'step': 500, 'train_loss': 4.977789878845215},\n",
              " {'step': 501, 'train_loss': 4.978320121765137},\n",
              " {'step': 502, 'train_loss': 4.9235663414001465},\n",
              " {'step': 503, 'train_loss': 5.032637119293213},\n",
              " {'step': 504, 'train_loss': 5.109720706939697},\n",
              " {'step': 505, 'train_loss': 5.1807451248168945},\n",
              " {'step': 506, 'train_loss': 5.0508222579956055},\n",
              " {'step': 507, 'train_loss': 5.016037940979004},\n",
              " {'step': 508, 'train_loss': 5.1780195236206055},\n",
              " {'step': 509, 'train_loss': 5.124094486236572},\n",
              " {'step': 510, 'train_loss': 5.054069995880127},\n",
              " {'step': 511, 'train_loss': 5.034177780151367},\n",
              " {'step': 512, 'train_loss': 4.958723545074463},\n",
              " {'step': 513, 'train_loss': 4.880174160003662},\n",
              " {'step': 514, 'train_loss': 4.825078964233398},\n",
              " {'step': 515, 'train_loss': 5.016269207000732},\n",
              " {'step': 516, 'train_loss': 4.974560737609863},\n",
              " {'step': 517, 'train_loss': 4.799953460693359},\n",
              " {'step': 518, 'train_loss': 5.080667018890381},\n",
              " {'step': 519, 'train_loss': 4.890087604522705},\n",
              " {'step': 520, 'train_loss': 4.807104110717773},\n",
              " {'step': 521, 'train_loss': 5.067512035369873},\n",
              " {'step': 522, 'train_loss': 5.047885894775391},\n",
              " {'step': 523, 'train_loss': 4.994065761566162},\n",
              " {'step': 524, 'train_loss': 5.068122863769531},\n",
              " {'step': 525, 'train_loss': 5.0230302810668945},\n",
              " {'step': 526, 'train_loss': 5.002374649047852},\n",
              " {'step': 527, 'train_loss': 5.163885593414307},\n",
              " {'step': 528, 'train_loss': 5.157695293426514},\n",
              " {'step': 529, 'train_loss': 5.066279888153076},\n",
              " {'step': 530, 'train_loss': 4.8883442878723145},\n",
              " {'step': 531, 'train_loss': 4.973006725311279},\n",
              " {'step': 532, 'train_loss': 5.201048851013184},\n",
              " {'step': 533, 'train_loss': 4.960421085357666},\n",
              " {'step': 534, 'train_loss': 4.966756343841553},\n",
              " {'step': 535, 'train_loss': 4.993124961853027},\n",
              " {'step': 536, 'train_loss': 4.9253010749816895},\n",
              " {'step': 537, 'train_loss': 4.954239845275879},\n",
              " {'step': 538, 'train_loss': 4.973170280456543},\n",
              " {'step': 539, 'train_loss': 5.002407550811768},\n",
              " {'step': 540, 'train_loss': 4.930235385894775},\n",
              " {'step': 541, 'train_loss': 4.976437091827393},\n",
              " {'step': 542, 'train_loss': 4.968847274780273},\n",
              " {'step': 543, 'train_loss': 4.97325325012207},\n",
              " {'step': 544, 'train_loss': 4.949077606201172},\n",
              " {'step': 545, 'train_loss': 5.105555534362793},\n",
              " {'step': 546, 'train_loss': 4.924804210662842},\n",
              " {'step': 547, 'train_loss': 4.904389381408691},\n",
              " {'step': 548, 'train_loss': 5.0476837158203125},\n",
              " {'step': 549, 'train_loss': 5.156867027282715},\n",
              " {'step': 550, 'train_loss': 5.01181697845459},\n",
              " {'step': 551, 'train_loss': 5.041390419006348},\n",
              " {'step': 552, 'train_loss': 5.03542947769165},\n",
              " {'step': 553, 'train_loss': 4.950960636138916},\n",
              " {'step': 554, 'train_loss': 5.106527328491211},\n",
              " {'step': 555, 'train_loss': 4.907662391662598},\n",
              " {'step': 556, 'train_loss': 5.009424686431885},\n",
              " {'step': 557, 'train_loss': 4.871612548828125},\n",
              " {'step': 558, 'train_loss': 5.002490997314453},\n",
              " {'step': 559, 'train_loss': 4.842870235443115},\n",
              " {'step': 560, 'train_loss': 4.888885021209717},\n",
              " {'step': 561, 'train_loss': 5.004605293273926},\n",
              " {'step': 562, 'train_loss': 5.056156158447266},\n",
              " {'step': 563, 'train_loss': 5.050524711608887},\n",
              " {'step': 564, 'train_loss': 4.852752208709717},\n",
              " {'step': 565, 'train_loss': 5.127012252807617},\n",
              " {'step': 566, 'train_loss': 5.086915016174316},\n",
              " {'step': 567, 'train_loss': 4.917850494384766},\n",
              " {'step': 568, 'train_loss': 4.946150779724121},\n",
              " {'step': 569, 'train_loss': 5.019569396972656},\n",
              " {'step': 570, 'train_loss': 4.7489542961120605},\n",
              " {'step': 571, 'train_loss': 4.915191173553467},\n",
              " {'step': 572, 'train_loss': 5.098698139190674},\n",
              " {'step': 573, 'train_loss': 4.999419689178467},\n",
              " {'step': 574, 'train_loss': 5.158798694610596},\n",
              " {'step': 575, 'train_loss': 5.031079292297363},\n",
              " {'step': 576, 'train_loss': 5.061727523803711},\n",
              " {'step': 577, 'train_loss': 4.797098636627197},\n",
              " {'step': 578, 'train_loss': 4.893768787384033},\n",
              " {'step': 579, 'train_loss': 4.9636006355285645},\n",
              " {'step': 580, 'train_loss': 4.9237589836120605},\n",
              " {'step': 581, 'train_loss': 4.984252452850342},\n",
              " {'step': 582, 'train_loss': 4.800009250640869},\n",
              " {'step': 583, 'train_loss': 4.881263732910156},\n",
              " {'step': 584, 'train_loss': 4.960326671600342},\n",
              " {'step': 585, 'train_loss': 4.921307563781738},\n",
              " {'step': 586, 'train_loss': 4.927623748779297},\n",
              " {'step': 587, 'train_loss': 4.93699836730957},\n",
              " {'step': 588, 'train_loss': 4.800010681152344},\n",
              " {'step': 589, 'train_loss': 4.922676086425781},\n",
              " {'step': 590, 'train_loss': 4.922836780548096},\n",
              " {'step': 591, 'train_loss': 4.870185375213623},\n",
              " {'step': 592, 'train_loss': 4.940258502960205},\n",
              " {'step': 593, 'train_loss': 4.912626266479492},\n",
              " {'step': 594, 'train_loss': 4.880251407623291},\n",
              " {'step': 595, 'train_loss': 5.0073018074035645},\n",
              " {'step': 596, 'train_loss': 4.949254989624023},\n",
              " {'step': 597, 'train_loss': 4.861274719238281},\n",
              " {'step': 598, 'train_loss': 4.933833599090576},\n",
              " {'step': 599, 'train_loss': 4.872341156005859},\n",
              " {'step': 600, 'train_loss': 4.9262919425964355},\n",
              " {'step': 601, 'train_loss': 5.059436798095703},\n",
              " {'step': 602, 'train_loss': 5.071960926055908},\n",
              " {'step': 603, 'train_loss': 4.998400688171387},\n",
              " {'step': 604, 'train_loss': 4.873552322387695},\n",
              " {'step': 605, 'train_loss': 4.856155872344971},\n",
              " {'step': 606, 'train_loss': 4.899999141693115},\n",
              " {'step': 607, 'train_loss': 4.90328311920166},\n",
              " {'step': 608, 'train_loss': 4.968526363372803},\n",
              " {'step': 609, 'train_loss': 4.836572170257568},\n",
              " {'step': 610, 'train_loss': 4.8698859214782715},\n",
              " {'step': 611, 'train_loss': 4.817019462585449},\n",
              " {'step': 612, 'train_loss': 4.964895248413086},\n",
              " {'step': 613, 'train_loss': 4.891618728637695},\n",
              " {'step': 614, 'train_loss': 4.944984436035156},\n",
              " {'step': 615, 'train_loss': 4.873797416687012},\n",
              " {'step': 616, 'train_loss': 4.8616719245910645},\n",
              " {'step': 617, 'train_loss': 5.0115180015563965},\n",
              " {'step': 618, 'train_loss': 4.917518138885498},\n",
              " {'step': 619, 'train_loss': 4.886629104614258},\n",
              " {'step': 620, 'train_loss': 5.0087761878967285},\n",
              " {'step': 621, 'train_loss': 4.889346122741699},\n",
              " {'step': 622, 'train_loss': 4.918785572052002},\n",
              " {'step': 623, 'train_loss': 5.024064064025879},\n",
              " {'step': 624, 'train_loss': 4.92282247543335},\n",
              " {'step': 625, 'train_loss': 4.891600608825684},\n",
              " {'step': 626, 'train_loss': 4.8728203773498535},\n",
              " {'step': 627, 'train_loss': 4.8233795166015625},\n",
              " {'step': 628, 'train_loss': 4.943695068359375},\n",
              " {'step': 629, 'train_loss': 4.853024482727051},\n",
              " {'step': 630, 'train_loss': 5.065876007080078},\n",
              " {'step': 631, 'train_loss': 5.069857120513916},\n",
              " {'step': 632, 'train_loss': 5.016073703765869},\n",
              " {'step': 633, 'train_loss': 5.041351318359375},\n",
              " {'step': 634, 'train_loss': 4.945940971374512},\n",
              " {'step': 635, 'train_loss': 4.961482048034668},\n",
              " {'step': 636, 'train_loss': 4.914465427398682},\n",
              " {'step': 637, 'train_loss': 5.028171539306641},\n",
              " {'step': 638, 'train_loss': 5.029642105102539},\n",
              " {'step': 639, 'train_loss': 5.0083794593811035},\n",
              " {'step': 640, 'train_loss': 4.903229236602783},\n",
              " {'step': 641, 'train_loss': 4.844906806945801},\n",
              " {'step': 642, 'train_loss': 4.936007499694824},\n",
              " {'step': 643, 'train_loss': 4.84823751449585},\n",
              " {'step': 644, 'train_loss': 4.803854465484619},\n",
              " {'step': 645, 'train_loss': 4.85464334487915},\n",
              " {'step': 646, 'train_loss': 4.885584354400635},\n",
              " {'step': 647, 'train_loss': 4.897927284240723},\n",
              " {'step': 648, 'train_loss': 4.927600860595703},\n",
              " {'step': 649, 'train_loss': 4.7541422843933105},\n",
              " {'step': 650, 'train_loss': 4.9117326736450195},\n",
              " {'step': 651, 'train_loss': 4.975740909576416},\n",
              " {'step': 652, 'train_loss': 4.81834077835083},\n",
              " {'step': 653, 'train_loss': 5.004146575927734},\n",
              " {'step': 654, 'train_loss': 4.871242523193359},\n",
              " {'step': 655, 'train_loss': 4.8181986808776855},\n",
              " {'step': 656, 'train_loss': 4.78465461730957},\n",
              " {'step': 657, 'train_loss': 4.8530988693237305},\n",
              " {'step': 658, 'train_loss': 4.935529708862305},\n",
              " {'step': 659, 'train_loss': 4.82033109664917},\n",
              " {'step': 660, 'train_loss': 4.767595291137695},\n",
              " {'step': 661, 'train_loss': 4.81071138381958},\n",
              " {'step': 662, 'train_loss': 4.808255195617676},\n",
              " {'step': 663, 'train_loss': 4.812199115753174},\n",
              " {'step': 664, 'train_loss': 4.932823181152344},\n",
              " {'step': 665, 'train_loss': 4.791365146636963},\n",
              " {'step': 666, 'train_loss': 4.9075164794921875},\n",
              " {'step': 667, 'train_loss': 4.767513751983643},\n",
              " {'step': 668, 'train_loss': 4.905014514923096},\n",
              " {'step': 669, 'train_loss': 4.834194183349609},\n",
              " {'step': 670, 'train_loss': 4.734471321105957},\n",
              " {'step': 671, 'train_loss': 4.823004722595215},\n",
              " {'step': 672, 'train_loss': 4.879371166229248},\n",
              " {'step': 673, 'train_loss': 4.9375224113464355},\n",
              " {'step': 674, 'train_loss': 5.001861095428467},\n",
              " {'step': 675, 'train_loss': 4.739859580993652},\n",
              " {'step': 676, 'train_loss': 4.82335901260376},\n",
              " {'step': 677, 'train_loss': 4.67854118347168},\n",
              " {'step': 678, 'train_loss': 4.896256446838379},\n",
              " {'step': 679, 'train_loss': 5.042855739593506},\n",
              " {'step': 680, 'train_loss': 4.881466865539551},\n",
              " {'step': 681, 'train_loss': 4.845956802368164},\n",
              " {'step': 682, 'train_loss': 4.920372009277344},\n",
              " {'step': 683, 'train_loss': 4.6515655517578125},\n",
              " {'step': 684, 'train_loss': 4.932341575622559},\n",
              " {'step': 685, 'train_loss': 4.871348857879639},\n",
              " {'step': 686, 'train_loss': 4.805248737335205},\n",
              " {'step': 687, 'train_loss': 4.764315605163574},\n",
              " {'step': 688, 'train_loss': 4.864315986633301},\n",
              " {'step': 689, 'train_loss': 4.788247108459473},\n",
              " {'step': 690, 'train_loss': 4.947995185852051},\n",
              " {'step': 691, 'train_loss': 4.944516181945801},\n",
              " {'step': 692, 'train_loss': 4.895016670227051},\n",
              " {'step': 693, 'train_loss': 4.634767532348633},\n",
              " {'step': 694, 'train_loss': 4.853531360626221},\n",
              " {'step': 695, 'train_loss': 4.868062496185303},\n",
              " {'step': 696, 'train_loss': 4.72724723815918},\n",
              " {'step': 697, 'train_loss': 4.803262710571289},\n",
              " {'step': 698, 'train_loss': 4.7533040046691895},\n",
              " {'step': 699, 'train_loss': 4.709687232971191},\n",
              " {'step': 700, 'train_loss': 4.8886895179748535},\n",
              " {'step': 701, 'train_loss': 4.702383995056152},\n",
              " {'step': 702, 'train_loss': 4.809493064880371},\n",
              " {'step': 703, 'train_loss': 4.844295024871826},\n",
              " {'step': 704, 'train_loss': 4.81089973449707},\n",
              " {'step': 705, 'train_loss': 4.96072244644165},\n",
              " {'step': 706, 'train_loss': 4.841848373413086},\n",
              " {'step': 707, 'train_loss': 4.922619342803955},\n",
              " {'step': 708, 'train_loss': 4.900825500488281},\n",
              " {'step': 709, 'train_loss': 4.703988552093506},\n",
              " {'step': 710, 'train_loss': 4.67334508895874},\n",
              " {'step': 711, 'train_loss': 4.816013813018799},\n",
              " {'step': 712, 'train_loss': 4.760804653167725},\n",
              " {'step': 713, 'train_loss': 4.829108238220215},\n",
              " {'step': 714, 'train_loss': 4.884246826171875},\n",
              " {'step': 715, 'train_loss': 4.927090167999268},\n",
              " {'step': 716, 'train_loss': 4.824532508850098},\n",
              " {'step': 717, 'train_loss': 4.836040019989014},\n",
              " {'step': 718, 'train_loss': 4.77356481552124},\n",
              " {'step': 719, 'train_loss': 4.883303642272949},\n",
              " {'step': 720, 'train_loss': 4.736125469207764},\n",
              " {'step': 721, 'train_loss': 4.924013137817383},\n",
              " {'step': 722, 'train_loss': 4.9179911613464355},\n",
              " {'step': 723, 'train_loss': 4.9151201248168945},\n",
              " {'step': 724, 'train_loss': 4.63739538192749},\n",
              " {'step': 725, 'train_loss': 4.684614181518555},\n",
              " {'step': 726, 'train_loss': 4.814596176147461},\n",
              " {'step': 727, 'train_loss': 4.737786769866943},\n",
              " {'step': 728, 'train_loss': 4.830571174621582},\n",
              " {'step': 729, 'train_loss': 4.738523483276367},\n",
              " {'step': 730, 'train_loss': 4.824923515319824},\n",
              " {'step': 731, 'train_loss': 5.02046012878418},\n",
              " {'step': 732, 'train_loss': 4.865375995635986},\n",
              " {'step': 733, 'train_loss': 4.822602272033691},\n",
              " {'step': 734, 'train_loss': 4.897233009338379},\n",
              " {'step': 735, 'train_loss': 4.762199878692627},\n",
              " {'step': 736, 'train_loss': 4.814811706542969},\n",
              " {'step': 737, 'train_loss': 4.999599933624268},\n",
              " {'step': 738, 'train_loss': 4.7997870445251465},\n",
              " {'step': 739, 'train_loss': 4.764865875244141},\n",
              " {'step': 740, 'train_loss': 4.739400863647461},\n",
              " {'step': 741, 'train_loss': 4.785422325134277},\n",
              " {'step': 742, 'train_loss': 4.903478145599365},\n",
              " {'step': 743, 'train_loss': 4.860511302947998},\n",
              " {'step': 744, 'train_loss': 4.68583345413208},\n",
              " {'step': 745, 'train_loss': 4.917549133300781},\n",
              " {'step': 746, 'train_loss': 4.816150665283203},\n",
              " {'step': 747, 'train_loss': 4.739798545837402},\n",
              " {'step': 748, 'train_loss': 4.814108371734619},\n",
              " {'step': 749, 'train_loss': 4.756093502044678},\n",
              " {'step': 750, 'train_loss': 4.751804351806641},\n",
              " {'step': 751, 'train_loss': 4.963374614715576},\n",
              " {'step': 752, 'train_loss': 4.784327983856201},\n",
              " {'step': 753, 'train_loss': 4.826559066772461},\n",
              " {'step': 754, 'train_loss': 4.728576183319092},\n",
              " {'step': 755, 'train_loss': 4.845527172088623},\n",
              " {'step': 756, 'train_loss': 4.7639689445495605},\n",
              " {'step': 757, 'train_loss': 4.803358554840088},\n",
              " {'step': 758, 'train_loss': 4.592474937438965},\n",
              " {'step': 759, 'train_loss': 4.822757720947266},\n",
              " {'step': 760, 'train_loss': 4.845363616943359},\n",
              " {'step': 761, 'train_loss': 4.883949279785156},\n",
              " {'step': 762, 'train_loss': 4.848867893218994},\n",
              " {'step': 763, 'train_loss': 4.824899196624756},\n",
              " {'step': 764, 'train_loss': 4.790734767913818},\n",
              " {'step': 765, 'train_loss': 4.808805465698242},\n",
              " {'step': 766, 'train_loss': 4.7342448234558105},\n",
              " {'step': 767, 'train_loss': 4.949429512023926},\n",
              " {'step': 768, 'train_loss': 4.986042499542236},\n",
              " {'step': 769, 'train_loss': 4.787067413330078},\n",
              " {'step': 770, 'train_loss': 4.7711920738220215},\n",
              " {'step': 771, 'train_loss': 4.662242889404297},\n",
              " {'step': 772, 'train_loss': 4.721757411956787},\n",
              " {'step': 773, 'train_loss': 4.928629398345947},\n",
              " {'step': 774, 'train_loss': 4.7573442459106445},\n",
              " {'step': 775, 'train_loss': 4.707421779632568},\n",
              " {'step': 776, 'train_loss': 4.855259895324707},\n",
              " {'step': 777, 'train_loss': 4.946381092071533},\n",
              " {'step': 778, 'train_loss': 4.7584123611450195},\n",
              " {'step': 779, 'train_loss': 4.919162750244141},\n",
              " {'step': 780, 'train_loss': 4.7576775550842285},\n",
              " {'step': 781, 'train_loss': 4.8721466064453125},\n",
              " {'step': 782, 'train_loss': 4.715890407562256},\n",
              " {'step': 783, 'train_loss': 4.8499369621276855},\n",
              " {'step': 784, 'train_loss': 4.813864707946777},\n",
              " {'step': 785, 'train_loss': 4.744193077087402},\n",
              " {'step': 786, 'train_loss': 4.910788059234619},\n",
              " {'step': 787, 'train_loss': 4.803667068481445},\n",
              " {'step': 788, 'train_loss': 4.818739891052246},\n",
              " {'step': 789, 'train_loss': 4.818401336669922},\n",
              " {'step': 790, 'train_loss': 4.877914905548096},\n",
              " {'step': 791, 'train_loss': 4.788551330566406},\n",
              " {'step': 792, 'train_loss': 4.581672668457031},\n",
              " {'step': 793, 'train_loss': 4.725655555725098},\n",
              " {'step': 794, 'train_loss': 4.440052032470703},\n",
              " {'step': 795, 'train_loss': 4.7598090171813965},\n",
              " {'step': 796, 'train_loss': 4.895411491394043},\n",
              " {'step': 797, 'train_loss': 4.776676654815674},\n",
              " {'step': 798, 'train_loss': 4.749563217163086},\n",
              " {'step': 799, 'train_loss': 4.773685932159424},\n",
              " {'step': 800, 'train_loss': 4.973230361938477},\n",
              " {'step': 801, 'train_loss': 4.867336750030518},\n",
              " {'step': 802, 'train_loss': 4.871286869049072},\n",
              " {'step': 803, 'train_loss': 4.704392910003662},\n",
              " {'step': 804, 'train_loss': 4.946804046630859},\n",
              " {'step': 805, 'train_loss': 4.792552471160889},\n",
              " {'step': 806, 'train_loss': 4.716226100921631},\n",
              " {'step': 807, 'train_loss': 4.600923538208008},\n",
              " {'step': 808, 'train_loss': 4.716732501983643},\n",
              " {'step': 809, 'train_loss': 4.75696325302124},\n",
              " {'step': 810, 'train_loss': 4.857464790344238},\n",
              " {'step': 811, 'train_loss': 4.822353839874268},\n",
              " {'step': 812, 'train_loss': 4.789158821105957},\n",
              " {'step': 813, 'train_loss': 4.717778205871582},\n",
              " {'step': 814, 'train_loss': 4.815674304962158},\n",
              " {'step': 815, 'train_loss': 4.837523937225342},\n",
              " {'step': 816, 'train_loss': 4.9073486328125},\n",
              " {'step': 817, 'train_loss': 4.81270170211792},\n",
              " {'step': 818, 'train_loss': 4.758237838745117},\n",
              " {'step': 819, 'train_loss': 4.913079261779785},\n",
              " {'step': 820, 'train_loss': 4.737768650054932},\n",
              " {'step': 821, 'train_loss': 4.908742904663086},\n",
              " {'step': 822, 'train_loss': 4.7926926612854},\n",
              " {'step': 823, 'train_loss': 4.763210296630859},\n",
              " {'step': 824, 'train_loss': 4.82732629776001},\n",
              " {'step': 825, 'train_loss': 4.685711860656738},\n",
              " {'step': 826, 'train_loss': 4.87658166885376},\n",
              " {'step': 827, 'train_loss': 4.733327388763428},\n",
              " {'step': 828, 'train_loss': 4.906244277954102},\n",
              " {'step': 829, 'train_loss': 4.738551139831543},\n",
              " {'step': 830, 'train_loss': 4.811468601226807},\n",
              " {'step': 831, 'train_loss': 4.615466117858887},\n",
              " {'step': 832, 'train_loss': 4.846179962158203},\n",
              " {'step': 833, 'train_loss': 4.800503730773926},\n",
              " {'step': 834, 'train_loss': 4.651414394378662},\n",
              " {'step': 835, 'train_loss': 4.727301597595215},\n",
              " {'step': 836, 'train_loss': 4.725016117095947},\n",
              " {'step': 837, 'train_loss': 4.6528191566467285},\n",
              " {'step': 838, 'train_loss': 4.828373432159424},\n",
              " {'step': 839, 'train_loss': 4.71514892578125},\n",
              " {'step': 840, 'train_loss': 4.7281174659729},\n",
              " {'step': 841, 'train_loss': 4.820094108581543},\n",
              " {'step': 842, 'train_loss': 4.6863298416137695},\n",
              " {'step': 843, 'train_loss': 4.706833839416504},\n",
              " {'step': 844, 'train_loss': 4.737392902374268},\n",
              " {'step': 845, 'train_loss': 4.867528915405273},\n",
              " {'step': 846, 'train_loss': 4.651278018951416},\n",
              " {'step': 847, 'train_loss': 4.7404704093933105},\n",
              " {'step': 848, 'train_loss': 4.7077131271362305},\n",
              " {'step': 849, 'train_loss': 4.696532249450684},\n",
              " {'step': 850, 'train_loss': 4.643098831176758},\n",
              " {'step': 851, 'train_loss': 4.787202835083008},\n",
              " {'step': 852, 'train_loss': 4.783825874328613},\n",
              " {'step': 853, 'train_loss': 4.8985161781311035},\n",
              " {'step': 854, 'train_loss': 4.766648292541504},\n",
              " {'step': 855, 'train_loss': 4.949474334716797},\n",
              " {'step': 856, 'train_loss': 4.8357391357421875},\n",
              " {'step': 857, 'train_loss': 4.749491214752197},\n",
              " {'step': 858, 'train_loss': 4.988775730133057},\n",
              " {'step': 859, 'train_loss': 4.628385543823242},\n",
              " {'step': 860, 'train_loss': 4.75705623626709},\n",
              " {'step': 861, 'train_loss': 4.703607082366943},\n",
              " {'step': 862, 'train_loss': 4.760298728942871},\n",
              " {'step': 863, 'train_loss': 4.753278732299805},\n",
              " {'step': 864, 'train_loss': 4.73444128036499},\n",
              " {'step': 865, 'train_loss': 4.93168830871582},\n",
              " {'step': 866, 'train_loss': 4.656091213226318},\n",
              " {'step': 867, 'train_loss': 4.8041582107543945},\n",
              " {'step': 868, 'train_loss': 4.763566017150879},\n",
              " {'step': 869, 'train_loss': 4.805486679077148},\n",
              " {'step': 870, 'train_loss': 4.641883373260498},\n",
              " {'step': 871, 'train_loss': 4.733224391937256},\n",
              " {'step': 872, 'train_loss': 4.6237382888793945},\n",
              " {'step': 873, 'train_loss': 4.707955837249756},\n",
              " {'step': 874, 'train_loss': 4.843612194061279},\n",
              " {'step': 875, 'train_loss': 4.718810081481934},\n",
              " {'step': 876, 'train_loss': 4.5871710777282715},\n",
              " {'step': 877, 'train_loss': 4.728589057922363},\n",
              " {'step': 878, 'train_loss': 4.6306610107421875},\n",
              " {'step': 879, 'train_loss': 4.765659809112549},\n",
              " {'step': 880, 'train_loss': 4.764143943786621},\n",
              " {'step': 881, 'train_loss': 4.852651596069336},\n",
              " {'step': 882, 'train_loss': 4.697822093963623},\n",
              " {'step': 883, 'train_loss': 4.683632850646973},\n",
              " {'step': 884, 'train_loss': 4.83239221572876},\n",
              " {'step': 885, 'train_loss': 4.611653804779053},\n",
              " {'step': 886, 'train_loss': 4.748926162719727},\n",
              " {'step': 887, 'train_loss': 4.631779193878174},\n",
              " {'step': 888, 'train_loss': 4.7331767082214355},\n",
              " {'step': 889, 'train_loss': 4.683620452880859},\n",
              " {'step': 890, 'train_loss': 4.628809452056885},\n",
              " {'step': 891, 'train_loss': 4.662234783172607},\n",
              " {'step': 892, 'train_loss': 4.794139385223389},\n",
              " {'step': 893, 'train_loss': 4.725749969482422},\n",
              " {'step': 894, 'train_loss': 4.857354640960693},\n",
              " {'step': 895, 'train_loss': 4.906673908233643},\n",
              " {'step': 896, 'train_loss': 4.8066277503967285},\n",
              " {'step': 897, 'train_loss': 4.6546478271484375},\n",
              " {'step': 898, 'train_loss': 4.695059299468994},\n",
              " {'step': 899, 'train_loss': 4.781772613525391},\n",
              " {'step': 900, 'train_loss': 4.782492637634277},\n",
              " {'step': 901, 'train_loss': 4.942199230194092},\n",
              " {'step': 902, 'train_loss': 4.577890396118164},\n",
              " {'step': 903, 'train_loss': 4.65969705581665},\n",
              " {'step': 904, 'train_loss': 4.639951229095459},\n",
              " {'step': 905, 'train_loss': 4.599673748016357},\n",
              " {'step': 906, 'train_loss': 4.919940948486328},\n",
              " {'step': 907, 'train_loss': 4.662766933441162},\n",
              " {'step': 908, 'train_loss': 4.5024213790893555},\n",
              " {'step': 909, 'train_loss': 4.936429977416992},\n",
              " {'step': 910, 'train_loss': 4.863910675048828},\n",
              " {'step': 911, 'train_loss': 4.765405654907227},\n",
              " {'step': 912, 'train_loss': 4.769711971282959},\n",
              " {'step': 913, 'train_loss': 4.7402191162109375},\n",
              " {'step': 914, 'train_loss': 4.885500431060791},\n",
              " {'step': 915, 'train_loss': 4.83290958404541},\n",
              " {'step': 916, 'train_loss': 4.578702449798584},\n",
              " {'step': 917, 'train_loss': 4.811155796051025},\n",
              " {'step': 918, 'train_loss': 4.92717170715332},\n",
              " {'step': 919, 'train_loss': 4.836104869842529},\n",
              " {'step': 920, 'train_loss': 4.882083892822266},\n",
              " {'step': 921, 'train_loss': 4.737152099609375},\n",
              " {'step': 922, 'train_loss': 4.804265975952148},\n",
              " {'step': 923, 'train_loss': 4.659135818481445},\n",
              " {'step': 924, 'train_loss': 4.769461631774902},\n",
              " {'step': 925, 'train_loss': 4.786387920379639},\n",
              " {'step': 926, 'train_loss': 4.772644996643066},\n",
              " {'step': 927, 'train_loss': 4.6775288581848145},\n",
              " {'step': 928, 'train_loss': 4.8153862953186035},\n",
              " {'step': 929, 'train_loss': 4.64382266998291},\n",
              " {'step': 930, 'train_loss': 4.867033958435059},\n",
              " {'step': 931, 'train_loss': 4.856082916259766},\n",
              " {'step': 932, 'train_loss': 4.877945423126221},\n",
              " {'step': 933, 'train_loss': 4.686275005340576},\n",
              " {'step': 934, 'train_loss': 4.769906997680664},\n",
              " {'step': 935, 'train_loss': 4.838961601257324},\n",
              " {'step': 936, 'train_loss': 4.671967029571533},\n",
              " {'step': 937, 'train_loss': 4.5379204750061035},\n",
              " {'step': 938, 'train_loss': 4.812243938446045},\n",
              " {'step': 939, 'train_loss': 4.863182067871094},\n",
              " {'step': 940, 'train_loss': 4.71045446395874},\n",
              " {'step': 941, 'train_loss': 4.714067459106445},\n",
              " {'step': 942, 'train_loss': 4.769529819488525},\n",
              " {'step': 943, 'train_loss': 4.824869632720947},\n",
              " {'step': 944, 'train_loss': 4.6067423820495605},\n",
              " {'step': 945, 'train_loss': 4.752866744995117},\n",
              " {'step': 946, 'train_loss': 4.6775288581848145},\n",
              " {'step': 947, 'train_loss': 4.733726978302002},\n",
              " {'step': 948, 'train_loss': 4.620345115661621},\n",
              " {'step': 949, 'train_loss': 4.702925205230713},\n",
              " {'step': 950, 'train_loss': 4.688531398773193},\n",
              " {'step': 951, 'train_loss': 4.633930683135986},\n",
              " {'step': 952, 'train_loss': 4.641327857971191},\n",
              " {'step': 953, 'train_loss': 4.725861549377441},\n",
              " {'step': 954, 'train_loss': 4.649229526519775},\n",
              " {'step': 955, 'train_loss': 4.7611002922058105},\n",
              " {'step': 956, 'train_loss': 4.6958746910095215},\n",
              " {'step': 957, 'train_loss': 4.54047966003418},\n",
              " {'step': 958, 'train_loss': 4.823179244995117},\n",
              " {'step': 959, 'train_loss': 4.859213352203369},\n",
              " {'step': 960, 'train_loss': 4.641848087310791},\n",
              " {'step': 961, 'train_loss': 4.65487003326416},\n",
              " {'step': 962, 'train_loss': 4.868044376373291},\n",
              " {'step': 963, 'train_loss': 4.767735481262207},\n",
              " {'step': 964, 'train_loss': 4.657695293426514},\n",
              " {'step': 965, 'train_loss': 4.81974458694458},\n",
              " {'step': 966, 'train_loss': 4.7981109619140625},\n",
              " {'step': 967, 'train_loss': 4.826697826385498},\n",
              " {'step': 968, 'train_loss': 4.671542644500732},\n",
              " {'step': 969, 'train_loss': 4.758530139923096},\n",
              " {'step': 970, 'train_loss': 4.7321085929870605},\n",
              " {'step': 971, 'train_loss': 4.588334560394287},\n",
              " {'step': 972, 'train_loss': 4.781353950500488},\n",
              " {'step': 973, 'train_loss': 4.770920276641846},\n",
              " {'step': 974, 'train_loss': 4.829546928405762},\n",
              " {'step': 975, 'train_loss': 4.718590259552002},\n",
              " {'step': 976, 'train_loss': 4.746920585632324},\n",
              " {'step': 977, 'train_loss': 4.64173698425293},\n",
              " {'step': 978, 'train_loss': 4.720598220825195},\n",
              " {'step': 979, 'train_loss': 4.6713385581970215},\n",
              " {'step': 980, 'train_loss': 4.585251331329346},\n",
              " {'step': 981, 'train_loss': 4.802215576171875},\n",
              " {'step': 982, 'train_loss': 4.807521820068359},\n",
              " {'step': 983, 'train_loss': 4.853146553039551},\n",
              " {'step': 984, 'train_loss': 4.8594536781311035},\n",
              " {'step': 985, 'train_loss': 4.679919242858887},\n",
              " {'step': 986, 'train_loss': 4.817830562591553},\n",
              " {'step': 987, 'train_loss': 4.744649887084961},\n",
              " {'step': 988, 'train_loss': 4.8365092277526855},\n",
              " {'step': 989, 'train_loss': 4.653956413269043},\n",
              " {'step': 990, 'train_loss': 4.860816478729248},\n",
              " {'step': 991, 'train_loss': 4.7883405685424805},\n",
              " {'step': 992, 'train_loss': 4.785364151000977},\n",
              " {'step': 993, 'train_loss': 4.613116264343262},\n",
              " {'step': 994, 'train_loss': 4.649063587188721},\n",
              " {'step': 995, 'train_loss': 4.818434715270996},\n",
              " {'step': 996, 'train_loss': 4.809483528137207},\n",
              " {'step': 997, 'train_loss': 4.729892730712891},\n",
              " {'step': 998, 'train_loss': 4.631223201751709},\n",
              " {'step': 999, 'train_loss': 4.810287952423096},\n",
              " {'step': 1000,\n",
              "  'train_loss': 4.82926607131958,\n",
              "  'valid_loss': 4.631684786932809},\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgO77jkVjDe4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aea12818-1d59-4999-e18b-1b600d180c3c"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(3, 5, requires_grad=True)\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "output = loss(input, target)\n",
        "print(output)\n",
        "output.backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.5508, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}